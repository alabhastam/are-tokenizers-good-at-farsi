{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f98a04",
   "metadata": {
    "papermill": {
     "duration": 0.002697,
     "end_time": "2025-11-02T09:19:14.683420",
     "exception": false,
     "start_time": "2025-11-02T09:19:14.680723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# description \n",
    "In this project, we are going to check different tokenizers and check how much they are efficient in tokenizing Persian (Farsi ) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba402071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:19:14.689402Z",
     "iopub.status.busy": "2025-11-02T09:19:14.689085Z",
     "iopub.status.idle": "2025-11-02T09:19:32.489374Z",
     "shell.execute_reply": "2025-11-02T09:19:32.488391Z"
    },
    "papermill": {
     "duration": 17.805146,
     "end_time": "2025-11-02T09:19:32.491154",
     "exception": false,
     "start_time": "2025-11-02T09:19:14.686008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from transformers import BertTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746c880",
   "metadata": {
    "papermill": {
     "duration": 0.001799,
     "end_time": "2025-11-02T09:19:32.495237",
     "exception": false,
     "start_time": "2025-11-02T09:19:32.493438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, we try the English format to be sure about how good the models are in the English model, and later we can compare it with Farsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c93947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:19:32.500930Z",
     "iopub.status.busy": "2025-11-02T09:19:32.500401Z",
     "iopub.status.idle": "2025-11-02T09:19:33.598603Z",
     "shell.execute_reply": "2025-11-02T09:19:33.597441Z"
    },
    "papermill": {
     "duration": 1.103208,
     "end_time": "2025-11-02T09:19:33.600393",
     "exception": false,
     "start_time": "2025-11-02T09:19:32.497185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT tokens: 10\n",
      "Token IDs: [791, 15740, 315, 4221, 4211, 706, 24411, 1268, 12966, 1131]\n",
      "\n",
      "Tokens → Text pieces:\n",
      " 1. ID    791 | 'The'\n",
      " 2. ID  15740 | ' evolution'\n",
      " 3. ID    315 | ' of'\n",
      " 4. ID   4221 | ' language'\n",
      " 5. ID   4211 | ' models'\n",
      " 6. ID    706 | ' has'\n",
      " 7. ID  24411 | ' transformed'\n",
      " 8. ID   1268 | ' how'\n",
      " 9. ID  12966 | ' humans'\n",
      "10. ID   1131 | '...'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "text = \"\"\"The evolution of language models has transformed how humans...\"\"\"\n",
    "\n",
    "# Load GPT‑4 / GPT‑3.5 tokenizer\n",
    "gpt_enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Encode text → list of integer token IDs\n",
    "gpt_tokens = gpt_enc.encode(text)\n",
    "\n",
    "# Show how many tokens\n",
    "print(\"GPT tokens:\", len(gpt_tokens))\n",
    "print(\"Token IDs:\", gpt_tokens)\n",
    "\n",
    "# Decode each token ID back to its actual text segment\n",
    "decoded_tokens = [gpt_enc.decode([t]) for t in gpt_tokens]\n",
    "print(\"\\nTokens → Text pieces:\")\n",
    "for i, (tok_id, tok_str) in enumerate(zip(gpt_tokens, decoded_tokens), 1):\n",
    "    print(f\"{i:>2}. ID {tok_id:>6} | '{tok_str}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a16b1",
   "metadata": {
    "papermill": {
     "duration": 0.002117,
     "end_time": "2025-11-02T09:19:33.607993",
     "exception": false,
     "start_time": "2025-11-02T09:19:33.605876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- cl100k_base is the Byte‑Pair‑Encoding (BPE) tokenizer used by modern OpenAI GPT models — specifically GPT‑4, GPT‑3.5‑Turbo, and text‑embedding‑3/5 families."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5a015",
   "metadata": {
    "papermill": {
     "duration": 0.002129,
     "end_time": "2025-11-02T09:19:33.612468",
     "exception": false,
     "start_time": "2025-11-02T09:19:33.610339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  How It Handles Farsi Internally\n",
    "- The merges in cl100k_base were trained mostly on English, Latin‑based code/text, so its merge rules focus on patterns common in those scripts.\n",
    "- Persian characters (e.g. «س», «ت», «م», etc.) are encoded in UTF-8 with 2 bytes each.If those specific byte sequences weren’t common in the English‑weighted corpus, the tokenizer will not merge them efficiently.\n",
    "- As a result, Persian text tends to produce many more tokens per word than English does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09b65bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:19:33.618112Z",
     "iopub.status.busy": "2025-11-02T09:19:33.617740Z",
     "iopub.status.idle": "2025-11-02T09:19:33.627656Z",
     "shell.execute_reply": "2025-11-02T09:19:33.625476Z"
    },
    "papermill": {
     "duration": 0.014954,
     "end_time": "2025-11-02T09:19:33.629571",
     "exception": false,
     "start_time": "2025-11-02T09:19:33.614617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 34\n",
      "\n",
      "Index | Token ID | Token Text\n",
      "----------------------------------------\n",
      "   1  |   14628 | 'ت'\n",
      "   2  |   30925 | 'ح'\n",
      "   3  |   73904 | 'ول'\n",
      "   4  |    8979 | ' �'\n",
      "   5  |     110 | '�'\n",
      "   6  |   22071 | 'ب'\n",
      "   7  |   40523 | 'ان'\n",
      "   8  |   90464 | '\\u200c'\n",
      "   9  |   16552 | 'ه'\n",
      "  10  |    5821 | 'ا'\n",
      "  11  |   82868 | ' به'\n",
      "  12  |   53257 | ' ش'\n",
      "  13  |   33411 | 'ک'\n",
      "  14  |    8700 | 'ل'\n",
      "  15  |   53257 | ' ش'\n",
      "  16  |   64832 | 'گ'\n",
      "  17  |   21604 | 'ف'\n",
      "  18  |   14628 | 'ت'\n",
      "  19  |   90464 | '\\u200c'\n",
      "  20  |   40523 | 'ان'\n",
      "  21  |   64832 | 'گ'\n",
      "  22  |   14728 | 'ی'\n",
      "  23  |   40797 | 'ز'\n",
      "  24  |   14728 | 'ی'\n",
      "  25  |   45430 | ' د'\n",
      "  26  |   64832 | 'گ'\n",
      "  27  |   11318 | 'ر'\n",
      "  28  |   64832 | 'گ'\n",
      "  29  |   12942 | 'و'\n",
      "  30  |   12061 | 'ن'\n",
      "  31  |   53257 | ' ش'\n",
      "  32  |   92435 | 'ده'\n",
      "  33  |   94253 | ' است'\n",
      "  34  |      13 | '.'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load GPT‑4 / GPT‑3.5 tokenizer\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Persian input text\n",
    "text_fa = \"تحول زبان‌ها به شکل شگفت‌انگیزی دگرگون شده است.\"\n",
    "\n",
    "# Encode to token IDs\n",
    "token_ids = enc.encode(text_fa)\n",
    "\n",
    "# Decode each token so we can see how Persian text is segmented\n",
    "decoded_tokens = [enc.decode([t]) for t in token_ids]\n",
    "\n",
    "print(f\"Total tokens: {len(token_ids)}\\n\")\n",
    "print(\"Index | Token ID | Token Text\")\n",
    "print(\"-\" * 40)\n",
    "for i, (tid, seg) in enumerate(zip(token_ids, decoded_tokens), start=1):\n",
    "    print(f\"{i:>4}  | {tid:>7} | {repr(seg)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb134f34",
   "metadata": {
    "papermill": {
     "duration": 0.001997,
     "end_time": "2025-11-02T09:19:33.634041",
     "exception": false,
     "start_time": "2025-11-02T09:19:33.632044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Since BPE learned mostly from ASCII/Latin text, it never saw those two Persian bytes often enough to merge them into large units — so the tokenizer only partially merges or leaves them split.That’s why you see micro‑segments like 'ت', 'ح', 'گ', etc.— each corresponds to one or two UTF‑8 bytes treated separately in the merge hierarchy."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.308648,
   "end_time": "2025-11-02T09:19:36.043835",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-02T09:19:09.735187",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
