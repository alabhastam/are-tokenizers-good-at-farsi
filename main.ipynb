{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/are-tokenizers-good-at-farsi?scriptVersionId=272858571\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"4f187995","metadata":{"papermill":{"duration":0.003111,"end_time":"2025-11-02T12:36:30.328215","exception":false,"start_time":"2025-11-02T12:36:30.325104","status":"completed"},"tags":[]},"source":["# description \n","In this project, we are going to check different tokenizers and check how much they are efficient in tokenizing Persian (Farsi ) words"]},{"cell_type":"code","execution_count":1,"id":"be9eb5a4","metadata":{"execution":{"iopub.execute_input":"2025-11-02T12:36:30.336288Z","iopub.status.busy":"2025-11-02T12:36:30.335855Z","iopub.status.idle":"2025-11-02T12:36:51.684294Z","shell.execute_reply":"2025-11-02T12:36:51.68317Z"},"papermill":{"duration":21.354669,"end_time":"2025-11-02T12:36:51.686196","exception":false,"start_time":"2025-11-02T12:36:30.331527","status":"completed"},"tags":[]},"outputs":[],"source":["import tiktoken\n","from transformers import BertTokenizer, AutoTokenizer"]},{"cell_type":"markdown","id":"5067f15c","metadata":{"papermill":{"duration":0.002624,"end_time":"2025-11-02T12:36:51.691648","exception":false,"start_time":"2025-11-02T12:36:51.689024","status":"completed"},"tags":[]},"source":["First, we try the English format to be sure about how good the models are in the English model, and later we can compare it with Farsi."]},{"cell_type":"code","execution_count":2,"id":"601c8424","metadata":{"execution":{"iopub.execute_input":"2025-11-02T12:36:51.6983Z","iopub.status.busy":"2025-11-02T12:36:51.697751Z","iopub.status.idle":"2025-11-02T12:36:52.371015Z","shell.execute_reply":"2025-11-02T12:36:52.36975Z"},"papermill":{"duration":0.678972,"end_time":"2025-11-02T12:36:52.373128","exception":false,"start_time":"2025-11-02T12:36:51.694156","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["GPT tokens: 10\n","Token IDs: [791, 15740, 315, 4221, 4211, 706, 24411, 1268, 12966, 1131]\n","\n","Tokens → Text pieces:\n"," 1. ID    791 | 'The'\n"," 2. ID  15740 | ' evolution'\n"," 3. ID    315 | ' of'\n"," 4. ID   4221 | ' language'\n"," 5. ID   4211 | ' models'\n"," 6. ID    706 | ' has'\n"," 7. ID  24411 | ' transformed'\n"," 8. ID   1268 | ' how'\n"," 9. ID  12966 | ' humans'\n","10. ID   1131 | '...'\n"]}],"source":["import tiktoken\n","\n","text = \"\"\"The evolution of language models has transformed how humans...\"\"\"\n","\n","# Load GPT‑4 / GPT‑3.5 tokenizer\n","gpt_enc = tiktoken.get_encoding(\"cl100k_base\")\n","\n","# Encode text → list of integer token IDs\n","gpt_tokens = gpt_enc.encode(text)\n","\n","# Show how many tokens\n","print(\"GPT tokens:\", len(gpt_tokens))\n","print(\"Token IDs:\", gpt_tokens)\n","\n","# Decode each token ID back to its actual text segment\n","decoded_tokens = [gpt_enc.decode([t]) for t in gpt_tokens]\n","print(\"\\nTokens → Text pieces:\")\n","for i, (tok_id, tok_str) in enumerate(zip(gpt_tokens, decoded_tokens), 1):\n","    print(f\"{i:>2}. ID {tok_id:>6} | '{tok_str}'\")\n"]},{"cell_type":"markdown","id":"1cd5e699","metadata":{"papermill":{"duration":0.002263,"end_time":"2025-11-02T12:36:52.378214","exception":false,"start_time":"2025-11-02T12:36:52.375951","status":"completed"},"tags":[]},"source":["- cl100k_base is the Byte‑Pair‑Encoding (BPE) tokenizer used by modern OpenAI GPT models — specifically GPT‑4, GPT‑3.5‑Turbo, and text‑embedding‑3/5 families."]},{"cell_type":"markdown","id":"ccbebb51","metadata":{"papermill":{"duration":0.00198,"end_time":"2025-11-02T12:36:52.38249","exception":false,"start_time":"2025-11-02T12:36:52.38051","status":"completed"},"tags":[]},"source":["#  How It Handles Farsi Internally\n","- The merges in cl100k_base were trained mostly on English, Latin‑based code/text, so its merge rules focus on patterns common in those scripts.\n","- Persian characters (e.g. «س», «ت», «م», etc.) are encoded in UTF-8 with 2 bytes each.If those specific byte sequences weren’t common in the English‑weighted corpus, the tokenizer will not merge them efficiently.\n","- As a result, Persian text tends to produce many more tokens per word than English does."]},{"cell_type":"code","execution_count":3,"id":"316aafdb","metadata":{"execution":{"iopub.execute_input":"2025-11-02T12:36:52.388202Z","iopub.status.busy":"2025-11-02T12:36:52.387896Z","iopub.status.idle":"2025-11-02T12:36:52.396691Z","shell.execute_reply":"2025-11-02T12:36:52.395254Z"},"papermill":{"duration":0.01387,"end_time":"2025-11-02T12:36:52.398499","exception":false,"start_time":"2025-11-02T12:36:52.384629","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Total tokens: 34\n","\n","Index | Token ID | Token Text\n","----------------------------------------\n","   1  |   14628 | 'ت'\n","   2  |   30925 | 'ح'\n","   3  |   73904 | 'ول'\n","   4  |    8979 | ' �'\n","   5  |     110 | '�'\n","   6  |   22071 | 'ب'\n","   7  |   40523 | 'ان'\n","   8  |   90464 | '\\u200c'\n","   9  |   16552 | 'ه'\n","  10  |    5821 | 'ا'\n","  11  |   82868 | ' به'\n","  12  |   53257 | ' ش'\n","  13  |   33411 | 'ک'\n","  14  |    8700 | 'ل'\n","  15  |   53257 | ' ش'\n","  16  |   64832 | 'گ'\n","  17  |   21604 | 'ف'\n","  18  |   14628 | 'ت'\n","  19  |   90464 | '\\u200c'\n","  20  |   40523 | 'ان'\n","  21  |   64832 | 'گ'\n","  22  |   14728 | 'ی'\n","  23  |   40797 | 'ز'\n","  24  |   14728 | 'ی'\n","  25  |   45430 | ' د'\n","  26  |   64832 | 'گ'\n","  27  |   11318 | 'ر'\n","  28  |   64832 | 'گ'\n","  29  |   12942 | 'و'\n","  30  |   12061 | 'ن'\n","  31  |   53257 | ' ش'\n","  32  |   92435 | 'ده'\n","  33  |   94253 | ' است'\n","  34  |      13 | '.'\n"]}],"source":["import tiktoken\n","\n","# Load GPT‑4 / GPT‑3.5 tokenizer\n","enc = tiktoken.get_encoding(\"cl100k_base\")\n","\n","# Persian input text\n","text_fa = \"تحول زبان‌ها به شکل شگفت‌انگیزی دگرگون شده است.\"\n","\n","# Encode to token IDs\n","token_ids = enc.encode(text_fa)\n","\n","# Decode each token so we can see how Persian text is segmented\n","decoded_tokens = [enc.decode([t]) for t in token_ids]\n","\n","print(f\"Total tokens: {len(token_ids)}\\n\")\n","print(\"Index | Token ID | Token Text\")\n","print(\"-\" * 40)\n","for i, (tid, seg) in enumerate(zip(token_ids, decoded_tokens), start=1):\n","    print(f\"{i:>4}  | {tid:>7} | {repr(seg)}\")\n"]},{"cell_type":"markdown","id":"11ca5a58","metadata":{"papermill":{"duration":0.002525,"end_time":"2025-11-02T12:36:52.403986","exception":false,"start_time":"2025-11-02T12:36:52.401461","status":"completed"},"tags":[]},"source":["# The Core Reason — Byte‑Level Encoding (Not Character‑Level)\n","- Since BPE learned mostly from ASCII/Latin text, it never saw those two Persian bytes often enough to merge them into large units — so the tokenizer only partially merges or leaves them split.That’s why you see micro‑segments like 'ت', 'ح', 'گ', etc.— each corresponds to one or two UTF‑8 bytes treated separately in the merge hierarchy.\n","- Every character in every language is first converted into its raw UTF‑8 byte sequence (1–4 bytes).\n","- During training, BPE repeatedly merges frequent byte sequences into tokens — but only those that occurred often in the English‑heavy training corpus:\n","  - English \"t\" is 1 byte (0x74).\n","  - Persian \"ت\" (U+062A) is 2 bytes (0xD8 0xAA)."]},{"cell_type":"markdown","id":"5f04ca25","metadata":{"papermill":{"duration":0.002016,"end_time":"2025-11-02T12:36:52.408304","exception":false,"start_time":"2025-11-02T12:36:52.406288","status":"completed"},"tags":[]},"source":["# Why Repeated Pieces like ' ش', 'گ', 'ان' Occur\n","- These are byte‑level merges that happened to form frequently during training, usually in transcribed or code‑mixed English data (where ‘g’, ‘an’, space+‘sh’ appear often).\n","- So the tokenizer reuses those merges on any UTF‑8 pattern that numerically matches similar byte patterns — even when the character visually corresponds to a Persian letter, because it doesn’t “understand” Unicode semantics.\n","\n","\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":30.610952,"end_time":"2025-11-02T12:36:55.312407","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-02T12:36:24.701455","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}