{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/are-tokenizers-good-at-farsi?scriptVersionId=272858091\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# description \nIn this project, we are going to check different tokenizers and check how much they are efficient in tokenizing Persian (Farsi ) words","metadata":{}},{"cell_type":"code","source":"import tiktoken\nfrom transformers import BertTokenizer, AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First, we try the English format to be sure about how good the models are in the English model, and later we can compare it with Farsi.","metadata":{}},{"cell_type":"code","source":"import tiktoken\n\ntext = \"\"\"The evolution of language models has transformed how humans...\"\"\"\n\n# Load GPTâ€‘4 / GPTâ€‘3.5 tokenizer\ngpt_enc = tiktoken.get_encoding(\"cl100k_base\")\n\n# Encode text â†’ list of integer token IDs\ngpt_tokens = gpt_enc.encode(text)\n\n# Show how many tokens\nprint(\"GPT tokens:\", len(gpt_tokens))\nprint(\"Token IDs:\", gpt_tokens)\n\n# Decode each token ID back to its actual text segment\ndecoded_tokens = [gpt_enc.decode([t]) for t in gpt_tokens]\nprint(\"\\nTokens â†’ Text pieces:\")\nfor i, (tok_id, tok_str) in enumerate(zip(gpt_tokens, decoded_tokens), 1):\n    print(f\"{i:>2}. ID {tok_id:>6} | '{tok_str}'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- cl100k_base is the Byteâ€‘Pairâ€‘Encoding (BPE) tokenizer used by modern OpenAI GPT models â€” specifically GPTâ€‘4, GPTâ€‘3.5â€‘Turbo, and textâ€‘embeddingâ€‘3/5 families.","metadata":{}},{"cell_type":"markdown","source":"#  How It Handles Farsi Internally\n- The merges in cl100k_base were trained mostly on English, Latinâ€‘based code/text, so its merge rules focus on patterns common in those scripts.\n- Persian characters (e.g. Â«Ø³Â», Â«ØªÂ», Â«Ù…Â», etc.) are encoded in UTF-8 with 2 bytes each.If those specific byte sequences werenâ€™t common in the Englishâ€‘weighted corpus, the tokenizer will not merge them efficiently.\n- As a result, Persian text tends to produce many more tokens per word than English does.","metadata":{}},{"cell_type":"code","source":"import tiktoken\n\n# Load GPTâ€‘4 / GPTâ€‘3.5 tokenizer\nenc = tiktoken.get_encoding(\"cl100k_base\")\n\n# Persian input text\ntext_fa = \"ØªØ­ÙˆÙ„ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø´Ú©Ù„ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ²ÛŒ Ø¯Ú¯Ø±Ú¯ÙˆÙ† Ø´Ø¯Ù‡ Ø§Ø³Øª.\"\n\n# Encode to token IDs\ntoken_ids = enc.encode(text_fa)\n\n# Decode each token so we can see how Persian text is segmented\ndecoded_tokens = [enc.decode([t]) for t in token_ids]\n\nprint(f\"Total tokens: {len(token_ids)}\\n\")\nprint(\"Index | Token ID | Token Text\")\nprint(\"-\" * 40)\nfor i, (tid, seg) in enumerate(zip(token_ids, decoded_tokens), start=1):\n    print(f\"{i:>4}  | {tid:>7} | {repr(seg)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The Core Reason â€” Byteâ€‘Level Encoding (Not Characterâ€‘Level)\n- Since BPE learned mostly from ASCII/Latin text, it never saw those two Persian bytes often enough to merge them into large units â€” so the tokenizer only partially merges or leaves them split.Thatâ€™s why you see microâ€‘segments like 'Øª', 'Ø­', 'Ú¯', etc.â€” each corresponds to one or two UTFâ€‘8 bytes treated separately in the merge hierarchy.\n- Every character in every language is first converted into its raw UTFâ€‘8 byte sequence (1â€“4 bytes).\n- During training, BPE repeatedly merges frequent byte sequences into tokens â€” but only those that occurred often in the Englishâ€‘heavy training corpus:\n  - English \"t\" is 1 byte (0x74).\n  - Persian \"Øª\" (U+062A) is 2 bytes (0xD8 0xAA).","metadata":{}},{"cell_type":"markdown","source":"# Why Repeated Pieces like ' Ø´', 'Ú¯', 'Ø§Ù†' Occur\n- These are byteâ€‘level merges that happened to form frequently during training, usually in transcribed or codeâ€‘mixed English data (where â€˜gâ€™, â€˜anâ€™, space+â€˜shâ€™ appear often).\n- So the tokenizer reuses those merges on any UTFâ€‘8 pattern that numerically matches similar byte patterns â€” even when the character visually corresponds to a Persian letter, because it doesnâ€™t â€œunderstandâ€ Unicode semantics.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Does Longer Persian Input Improve Tokenization Accuracy?\nOnly slightly, and in a statistical sense â€” not linguistically. Because: \n- GPTâ€™s cl100k_base tokenizer has a fixed preâ€‘trained merge table (â‰ˆâ€¯100â€¯k merges).\n- It wonâ€™t â€œlearnâ€ or adapt midâ€‘runtime when you feed longer text â€” it always follows the same UTFâ€‘8 byteâ€‘merge rules.\n- However, in longer text, Persian letter combinations that happen to repeat might match longer merge patterns in the table (e.g., \"Ø§Ù†\"â€¯orâ€¯\"Ù…ÛŒ\").So you may get marginally longer tokens and a slightly smaller total token count ratio.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tiktoken\n\n# Load tokenizer\nenc = tiktoken.get_encoding(\"cl100k_base\")\n\n# Your longer Persian paragraph\ntext_fa = (\n    \"Ø¯Ø± Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù†Ù‚Ø´ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…ÛŒ Ø¯Ø± ØªØ­ÙˆÙ„Ø§Øª ÙÙ†Ø§ÙˆØ±ÛŒ Ø§ÛŒÙØ§ Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯. \"\n    \"Ø§ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ú©Ù†ÙˆÙ† Ù‚Ø§Ø¯Ø±Ù†Ø¯ Ù…ØªÙˆÙ† Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ Ø¯Ø±Ú© Ú©Ù†Ù†Ø¯ØŒ Ø¨Ù‡ Ø³Ø¤Ø§Ù„Ø§Øª Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù†Ø¯ Ùˆ Ø­ØªÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ø´Ø¨ÛŒÙ‡ Ø§Ù†Ø³Ø§Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù†Ø¯. \"\n    \"Ù¾ÛŒØ´Ø±ÙØªâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¸ÛŒÙ… Ø¯Ø± Ø­ÙˆØ²Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ØŒ Ø¨Ø§Ø¹Ø« Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ØªÙˆØ§Ù†Ø§ÛŒÛŒ ØªØ­Ù„ÛŒÙ„ Ø²Ø¨Ø§Ù†ØŒ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ØŒ \"\n    \"Ùˆ Ø¯Ø±Ú© Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ù†Ø³Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ø§ Ø¯Ù‚ØªÛŒ Ø¨ÛŒâ€ŒØ³Ø§Ø¨Ù‚Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¨Ú¯Ø°Ø§Ø±Ù†Ø¯. \"\n    \"Ø§Ù…Ø±ÙˆØ²Ù‡ Ø§ÛŒÙ† ÙÙ†Ø§ÙˆØ±ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒØŒ Ú¯ÙØªâ€ŒÙˆÚ¯Ùˆ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±ØŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¬Ø§Ø²ÛŒ Ùˆ Ø­ØªÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ Ø¯Ø± Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ú©Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. \"\n    \"Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø³ØªØ±Ø¯Ù‡ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø²Ø±Ú¯ØŒ Ù…Ø³ÛŒØ± Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù†Ø³Ø§Ù† Ùˆ Ù…Ø§Ø´ÛŒÙ† Ø±Ø§ Ù…ØªØ­ÙˆÙ„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.\"\n)\n\n\n# Tokenize and decode\ntoken_ids = enc.encode(text_fa)\ndecoded_tokens = [enc.decode([t]) for t in token_ids]\n\n# Create concise DataFrame\ndf = pd.DataFrame({\n    \"Index\": range(1, len(token_ids)+1),\n    \"Token_ID\": token_ids,\n    \"Decoded\": decoded_tokens\n})\n\nprint(f\"Total tokens: {len(token_ids)}\")\ndf.head(20).style.set_properties(**{\"text-align\": \"left\"})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- It has about 207 tokens we are not gonna show all of them because makes the  notebook overwhelming.","metadata":{}},{"cell_type":"markdown","source":"# Using specific transformer and tokenizers for better accuracy\nGoal: Demonstrate how multilingual models (XLMâ€‘R and mBERT) improve token efficiency in Persian vs GPTâ€™s byteâ€‘level tokenizer.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom transformers import AutoTokenizer\nimport tiktoken\n\n# Disable chat template checks for Hugging Face >=4.53.0\nos.environ[\"HF_HUB_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_RESUME_DOWNLOADS\"] = \"1\"\n\n# === Input: 100-word Persian text ===\ntext_fa = (\n    \"Ø¯Ø± Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù†Ù‚Ø´ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…ÛŒ Ø¯Ø± ØªØ­ÙˆÙ„Ø§Øª ÙÙ†Ø§ÙˆØ±ÛŒ Ø§ÛŒÙØ§ Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯. \"\n    \"Ø§ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ú©Ù†ÙˆÙ† Ù‚Ø§Ø¯Ø±Ù†Ø¯ Ù…ØªÙˆÙ† Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ Ø¯Ø±Ú© Ú©Ù†Ù†Ø¯ØŒ Ø¨Ù‡ Ø³Ø¤Ø§Ù„Ø§Øª Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù†Ø¯ Ùˆ Ø­ØªÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ø´Ø¨ÛŒÙ‡ Ø§Ù†Ø³Ø§Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù†Ø¯. \"\n    \"Ù¾ÛŒØ´Ø±ÙØªâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¸ÛŒÙ… Ø¯Ø± Ø­ÙˆØ²Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ØŒ Ø¨Ø§Ø¹Ø« Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ØªÙˆØ§Ù†Ø§ÛŒÛŒ ØªØ­Ù„ÛŒÙ„ Ø²Ø¨Ø§Ù†ØŒ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ØŒ \"\n    \"Ùˆ Ø¯Ø±Ú© Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ù†Ø³Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ø§ Ø¯Ù‚ØªÛŒ Ø¨ÛŒâ€ŒØ³Ø§Ø¨Ù‚Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¨Ú¯Ø°Ø§Ø±Ù†Ø¯. \"\n    \"Ø§Ù…Ø±ÙˆØ²Ù‡ Ø§ÛŒÙ† ÙÙ†Ø§ÙˆØ±ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù…Ø§Ø´ÛŒÙ†ÛŒØŒ Ú¯ÙØªâ€ŒÙˆÚ¯Ùˆ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±ØŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¬Ø§Ø²ÛŒ Ùˆ Ø­ØªÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ Ø¯Ø± Ø±Ø³Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ú©Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. \"\n    \"Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø³ØªØ±Ø¯Ù‡ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ø¨Ø²Ø±Ú¯ØŒ Ù…Ø³ÛŒØ± Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù†Ø³Ø§Ù† Ùˆ Ù…Ø§Ø´ÛŒÙ† Ø±Ø§ Ù…ØªØ­ÙˆÙ„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.\"\n)\n\n# === 1. GPT Tokenizer (cl100k_base) ===\nenc = tiktoken.get_encoding(\"cl100k_base\")\ntokens_gpt = enc.encode(text_fa)\ndecoded_gpt = [enc.decode([t]) for t in tokens_gpt]\n\n# === 2. XLMâ€‘RoBERTa Tokenizer ===\nxlmr_tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\ntokens_xlmr = xlmr_tok.tokenize(text_fa)\n\n# === 3. Multilingual BERT Tokenizer ===\nmbert_tok = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\ntokens_mbert = mbert_tok.tokenize(text_fa)\n\n# === Create token count summary ===\nsummary = pd.DataFrame({\n    \"Tokenizer\": [\"GPT (cl100k_base)\", \"XLM-R (SentencePiece)\", \"mBERT (WordPiece)\"],\n    \"Algorithm\": [\"Byteâ€‘BPE (UTFâ€‘8)\", \"SentencePiece BPE\", \"WordPiece\"],\n    \"Token Count\": [len(tokens_gpt), len(tokens_xlmr), len(tokens_mbert)]\n})\nsummary[\"Tokens per Word (â‰ˆ100 words)\"] = (summary[\"Token Count\"] / 100).round(2)\n\nprint(\"ğŸ“Š Tokenization Summary:\\n\")\ndisplay(summary.style.set_properties(**{\"text-align\": \"center\"}))\n\n# === Optional: preview first few tokens from each ===\npreview_len = 15\nprint(\"\\nGPT sample tokens:\", decoded_gpt[:preview_len])\nprint(\"XLM-R sample tokens:\", tokens_xlmr[:preview_len])\nprint(\"mBERT sample tokens:\", tokens_mbert[:preview_len])\n\n# === Export full tokens for side-by-side inspection ===\npd.DataFrame({\n    \"GPT_cl100k\": decoded_gpt,\n}).to_csv(\"/kaggle/working/gpt_farsi_tokens.csv\", index=False)\n\npd.DataFrame({\n    \"XLMR_tokens\": tokens_xlmr,\n}).to_csv(\"/kaggle/working/xlmr_farsi_tokens.csv\", index=False)\n\npd.DataFrame({\n    \"mBERT_tokens\": tokens_mbert,\n}).to_csv(\"/kaggle/working/mbert_farsi_tokens.csv\", index=False)\nprint(\"\\nâœ… Full token lists saved as CSV files in /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:17:33.783467Z","iopub.execute_input":"2025-11-02T13:17:33.788688Z","iopub.status.idle":"2025-11-02T13:17:39.776586Z","shell.execute_reply.started":"2025-11-02T13:17:33.788626Z","shell.execute_reply":"2025-11-02T13:17:39.775332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a66dc08378438da23086fcd61b8fe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7a391058a540989adcc7d2025da1da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9183d2a9884541b772540762648d74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0ce0d9ef3d467282d752fad2a00c2b"}},"metadata":{}},{"name":"stdout","text":"ğŸ“Š Tokenization Summary:\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7ebedd4583d0>","text/html":"<style type=\"text/css\">\n#T_359cd_row0_col0, #T_359cd_row0_col1, #T_359cd_row0_col2, #T_359cd_row0_col3, #T_359cd_row1_col0, #T_359cd_row1_col1, #T_359cd_row1_col2, #T_359cd_row1_col3, #T_359cd_row2_col0, #T_359cd_row2_col1, #T_359cd_row2_col2, #T_359cd_row2_col3 {\n  text-align: center;\n}\n</style>\n<table id=\"T_359cd\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_359cd_level0_col0\" class=\"col_heading level0 col0\" >Tokenizer</th>\n      <th id=\"T_359cd_level0_col1\" class=\"col_heading level0 col1\" >Algorithm</th>\n      <th id=\"T_359cd_level0_col2\" class=\"col_heading level0 col2\" >Token Count</th>\n      <th id=\"T_359cd_level0_col3\" class=\"col_heading level0 col3\" >Tokens per Word (â‰ˆ100 words)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_359cd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_359cd_row0_col0\" class=\"data row0 col0\" >GPT (cl100k_base)</td>\n      <td id=\"T_359cd_row0_col1\" class=\"data row0 col1\" >Byteâ€‘BPE (UTFâ€‘8)</td>\n      <td id=\"T_359cd_row0_col2\" class=\"data row0 col2\" >409</td>\n      <td id=\"T_359cd_row0_col3\" class=\"data row0 col3\" >4.090000</td>\n    </tr>\n    <tr>\n      <th id=\"T_359cd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_359cd_row1_col0\" class=\"data row1 col0\" >XLM-R (SentencePiece)</td>\n      <td id=\"T_359cd_row1_col1\" class=\"data row1 col1\" >SentencePiece BPE</td>\n      <td id=\"T_359cd_row1_col2\" class=\"data row1 col2\" >142</td>\n      <td id=\"T_359cd_row1_col3\" class=\"data row1 col3\" >1.420000</td>\n    </tr>\n    <tr>\n      <th id=\"T_359cd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_359cd_row2_col0\" class=\"data row2 col0\" >mBERT (WordPiece)</td>\n      <td id=\"T_359cd_row2_col1\" class=\"data row2 col1\" >WordPiece</td>\n      <td id=\"T_359cd_row2_col2\" class=\"data row2 col2\" >186</td>\n      <td id=\"T_359cd_row2_col3\" class=\"data row2 col3\" >1.860000</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}},{"name":"stdout","text":"\nGPT sample tokens: ['Ø¯', 'Ø±', ' Ø³', 'Ø§Ù„', '\\u200c', 'Ù‡', 'Ø§ÛŒ', ' Ø§', 'Ø®', 'ÛŒØ±', 'ØŒ', ' Ù…', 'Ø¯', 'Ù„', '\\u200c']\nXLM-R sample tokens: ['â–Ø¯Ø±', 'â–Ø³Ø§Ù„', 'â–Ù‡Ø§ÛŒ', 'â–Ø§Ø®ÛŒØ±', 'ØŒ', 'â–Ù…Ø¯Ù„', 'â–Ù‡Ø§ÛŒ', 'â–Ø²Ø¨Ø§Ù†', 'ÛŒ', 'â–Ù¾ÛŒØ´Ø±ÙØªÙ‡', 'â–Ù†Ù‚Ø´', 'â–Ø¨Ø³ÛŒØ§Ø±', 'â–Ù…Ù‡Ù…ÛŒ', 'â–Ø¯Ø±', 'â–ØªØ­ÙˆÙ„Ø§Øª']\nmBERT sample tokens: ['Ø¯Ø±', 'Ø³Ø§Ù„Ù‡Ø§ÛŒ', 'Ø§', '##Ø®ÛŒØ±', 'ØŒ', 'Ù…Ø¯Ù„', '##Ù‡Ø§ÛŒ', 'Ø²Ø¨Ø§Ù†', '##ÛŒ', 'Ù¾ÛŒØ´', '##Ø±ÙØª', '##Ù‡', 'Ù†Ù‚Ø´', 'Ø¨Ø³ÛŒØ§Ø±', 'Ù…Ù‡Ù…']\n\nâœ… Full token lists saved as CSV files in /kaggle/working/\n","output_type":"stream"}],"execution_count":2}]}