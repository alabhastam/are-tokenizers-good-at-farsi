{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/are-tokenizers-good-at-farsi?scriptVersionId=272861429\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"5cadee03","metadata":{"papermill":{"duration":0.003078,"end_time":"2025-11-02T12:49:47.581536","exception":false,"start_time":"2025-11-02T12:49:47.578458","status":"completed"},"tags":[]},"source":["# description \n","In this project, we are going to check different tokenizers and check how much they are efficient in tokenizing Persian (Farsi ) words"]},{"cell_type":"code","execution_count":1,"id":"a2f572ac","metadata":{"execution":{"iopub.execute_input":"2025-11-02T12:49:47.58969Z","iopub.status.busy":"2025-11-02T12:49:47.589306Z","iopub.status.idle":"2025-11-02T12:50:04.698806Z","shell.execute_reply":"2025-11-02T12:50:04.697141Z"},"papermill":{"duration":17.115391,"end_time":"2025-11-02T12:50:04.700711","exception":false,"start_time":"2025-11-02T12:49:47.58532","status":"completed"},"tags":[]},"outputs":[],"source":["import tiktoken\n","from transformers import BertTokenizer, AutoTokenizer"]},{"cell_type":"markdown","id":"f28a8eb1","metadata":{"papermill":{"duration":0.002181,"end_time":"2025-11-02T12:50:04.705636","exception":false,"start_time":"2025-11-02T12:50:04.703455","status":"completed"},"tags":[]},"source":["First, we try the English format to be sure about how good the models are in the English model, and later we can compare it with Farsi."]},{"cell_type":"code","execution_count":2,"id":"33f6544f","metadata":{"execution":{"iopub.execute_input":"2025-11-02T12:50:04.71345Z","iopub.status.busy":"2025-11-02T12:50:04.712452Z","iopub.status.idle":"2025-11-02T12:50:05.669255Z","shell.execute_reply":"2025-11-02T12:50:05.668182Z"},"papermill":{"duration":0.962232,"end_time":"2025-11-02T12:50:05.670819","exception":false,"start_time":"2025-11-02T12:50:04.708587","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["GPT tokens: 10\n","Token IDs: [791, 15740, 315, 4221, 4211, 706, 24411, 1268, 12966, 1131]\n","\n","Tokens → Text pieces:\n"," 1. ID    791 | 'The'\n"," 2. ID  15740 | ' evolution'\n"," 3. ID    315 | ' of'\n"," 4. ID   4221 | ' language'\n"," 5. ID   4211 | ' models'\n"," 6. ID    706 | ' has'\n"," 7. ID  24411 | ' transformed'\n"," 8. ID   1268 | ' how'\n"," 9. ID  12966 | ' humans'\n","10. ID   1131 | '...'\n"]}],"source":["import tiktoken\n","\n","text = \"\"\"The evolution of language models has transformed how humans...\"\"\"\n","\n","# Load GPT‑4 / GPT‑3.5 tokenizer\n","gpt_enc = tiktoken.get_encoding(\"cl100k_base\")\n","\n","# Encode text → list of integer token IDs\n","gpt_tokens = gpt_enc.encode(text)\n","\n","# Show how many tokens\n","print(\"GPT tokens:\", len(gpt_tokens))\n","print(\"Token IDs:\", gpt_tokens)\n","\n","# Decode each token ID back to its actual text segment\n","decoded_tokens = [gpt_enc.decode([t]) for t in gpt_tokens]\n","print(\"\\nTokens → Text pieces:\")\n","for i, (tok_id, tok_str) in enumerate(zip(gpt_tokens, decoded_tokens), 1):\n","    print(f\"{i:>2}. ID {tok_id:>6} | '{tok_str}'\")\n"]},{"cell_type":"markdown","id":"29ed90f6","metadata":{"papermill":{"duration":0.002502,"end_time":"2025-11-02T12:50:05.675975","exception":false,"start_time":"2025-11-02T12:50:05.673473","status":"completed"},"tags":[]},"source":["- cl100k_base is the Byte‑Pair‑Encoding (BPE) tokenizer used by modern OpenAI GPT models — specifically GPT‑4, GPT‑3.5‑Turbo, and text‑embedding‑3/5 families."]},{"cell_type":"markdown","id":"2d404010","metadata":{"papermill":{"duration":0.002157,"end_time":"2025-11-02T12:50:05.680539","exception":false,"start_time":"2025-11-02T12:50:05.678382","status":"completed"},"tags":[]},"source":["#  How It Handles Farsi Internally\n","- The merges in cl100k_base were trained mostly on English, Latin‑based code/text, so its merge rules focus on patterns common in those scripts.\n","- Persian characters (e.g. «س», «ت», «م», etc.) are encoded in UTF-8 with 2 bytes each.If those specific byte sequences weren’t common in the English‑weighted corpus, the tokenizer will not merge them efficiently.\n","- As a result, Persian text tends to produce many more tokens per word than English does."]},{"cell_type":"code","execution_count":3,"id":"ac62cb1e","metadata":{"execution":{"iopub.execute_input":"2025-11-02T12:50:05.686687Z","iopub.status.busy":"2025-11-02T12:50:05.686352Z","iopub.status.idle":"2025-11-02T12:50:05.69575Z","shell.execute_reply":"2025-11-02T12:50:05.694652Z"},"papermill":{"duration":0.014149,"end_time":"2025-11-02T12:50:05.697157","exception":false,"start_time":"2025-11-02T12:50:05.683008","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Total tokens: 34\n","\n","Index | Token ID | Token Text\n","----------------------------------------\n","   1  |   14628 | 'ت'\n","   2  |   30925 | 'ح'\n","   3  |   73904 | 'ول'\n","   4  |    8979 | ' �'\n","   5  |     110 | '�'\n","   6  |   22071 | 'ب'\n","   7  |   40523 | 'ان'\n","   8  |   90464 | '\\u200c'\n","   9  |   16552 | 'ه'\n","  10  |    5821 | 'ا'\n","  11  |   82868 | ' به'\n","  12  |   53257 | ' ش'\n","  13  |   33411 | 'ک'\n","  14  |    8700 | 'ل'\n","  15  |   53257 | ' ش'\n","  16  |   64832 | 'گ'\n","  17  |   21604 | 'ف'\n","  18  |   14628 | 'ت'\n","  19  |   90464 | '\\u200c'\n","  20  |   40523 | 'ان'\n","  21  |   64832 | 'گ'\n","  22  |   14728 | 'ی'\n","  23  |   40797 | 'ز'\n","  24  |   14728 | 'ی'\n","  25  |   45430 | ' د'\n","  26  |   64832 | 'گ'\n","  27  |   11318 | 'ر'\n","  28  |   64832 | 'گ'\n","  29  |   12942 | 'و'\n","  30  |   12061 | 'ن'\n","  31  |   53257 | ' ش'\n","  32  |   92435 | 'ده'\n","  33  |   94253 | ' است'\n","  34  |      13 | '.'\n"]}],"source":["import tiktoken\n","\n","# Load GPT‑4 / GPT‑3.5 tokenizer\n","enc = tiktoken.get_encoding(\"cl100k_base\")\n","\n","# Persian input text\n","text_fa = \"تحول زبان‌ها به شکل شگفت‌انگیزی دگرگون شده است.\"\n","\n","# Encode to token IDs\n","token_ids = enc.encode(text_fa)\n","\n","# Decode each token so we can see how Persian text is segmented\n","decoded_tokens = [enc.decode([t]) for t in token_ids]\n","\n","print(f\"Total tokens: {len(token_ids)}\\n\")\n","print(\"Index | Token ID | Token Text\")\n","print(\"-\" * 40)\n","for i, (tid, seg) in enumerate(zip(token_ids, decoded_tokens), start=1):\n","    print(f\"{i:>4}  | {tid:>7} | {repr(seg)}\")\n"]},{"cell_type":"markdown","id":"63075f0b","metadata":{"papermill":{"duration":0.002351,"end_time":"2025-11-02T12:50:05.702313","exception":false,"start_time":"2025-11-02T12:50:05.699962","status":"completed"},"tags":[]},"source":["# The Core Reason — Byte‑Level Encoding (Not Character‑Level)\n","- Since BPE learned mostly from ASCII/Latin text, it never saw those two Persian bytes often enough to merge them into large units — so the tokenizer only partially merges or leaves them split.That’s why you see micro‑segments like 'ت', 'ح', 'گ', etc.— each corresponds to one or two UTF‑8 bytes treated separately in the merge hierarchy.\n","- Every character in every language is first converted into its raw UTF‑8 byte sequence (1–4 bytes).\n","- During training, BPE repeatedly merges frequent byte sequences into tokens — but only those that occurred often in the English‑heavy training corpus:\n","  - English \"t\" is 1 byte (0x74).\n","  - Persian \"ت\" (U+062A) is 2 bytes (0xD8 0xAA)."]},{"cell_type":"markdown","id":"528e45a5","metadata":{"papermill":{"duration":0.002251,"end_time":"2025-11-02T12:50:05.707146","exception":false,"start_time":"2025-11-02T12:50:05.704895","status":"completed"},"tags":[]},"source":["# Why Repeated Pieces like ' ش', 'گ', 'ان' Occur\n","- These are byte‑level merges that happened to form frequently during training, usually in transcribed or code‑mixed English data (where ‘g’, ‘an’, space+‘sh’ appear often).\n","- So the tokenizer reuses those merges on any UTF‑8 pattern that numerically matches similar byte patterns — even when the character visually corresponds to a Persian letter, because it doesn’t “understand” Unicode semantics.\n","\n","\n"]},{"cell_type":"markdown","id":"e3c97985","metadata":{"papermill":{"duration":0.002451,"end_time":"2025-11-02T12:50:05.712692","exception":false,"start_time":"2025-11-02T12:50:05.710241","status":"completed"},"tags":[]},"source":["# Does Longer Persian Input Improve Tokenization Accuracy?\n","Only slightly, and in a statistical sense — not linguistically. Because: \n","- GPT’s cl100k_base tokenizer has a fixed pre‑trained merge table (≈ 100 k merges).\n","- It won’t “learn” or adapt mid‑runtime when you feed longer text — it always follows the same UTF‑8 byte‑merge rules.\n","- However, in longer text, Persian letter combinations that happen to repeat might match longer merge patterns in the table (e.g., \"ان\" or \"می\").So you may get marginally longer tokens and a slightly smaller total token count ratio.\n"]},{"cell_type":"code","execution_count":4,"id":"252e66b6","metadata":{"execution":{"iopub.execute_input":"2025-11-02T12:50:05.719204Z","iopub.status.busy":"2025-11-02T12:50:05.71885Z","iopub.status.idle":"2025-11-02T12:50:07.701301Z","shell.execute_reply":"2025-11-02T12:50:07.700389Z"},"papermill":{"duration":1.987636,"end_time":"2025-11-02T12:50:07.702929","exception":false,"start_time":"2025-11-02T12:50:05.715293","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Total tokens: 206\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_d1c3f_row0_col0, #T_d1c3f_row0_col1, #T_d1c3f_row0_col2, #T_d1c3f_row1_col0, #T_d1c3f_row1_col1, #T_d1c3f_row1_col2, #T_d1c3f_row2_col0, #T_d1c3f_row2_col1, #T_d1c3f_row2_col2, #T_d1c3f_row3_col0, #T_d1c3f_row3_col1, #T_d1c3f_row3_col2, #T_d1c3f_row4_col0, #T_d1c3f_row4_col1, #T_d1c3f_row4_col2, #T_d1c3f_row5_col0, #T_d1c3f_row5_col1, #T_d1c3f_row5_col2, #T_d1c3f_row6_col0, #T_d1c3f_row6_col1, #T_d1c3f_row6_col2, #T_d1c3f_row7_col0, #T_d1c3f_row7_col1, #T_d1c3f_row7_col2, #T_d1c3f_row8_col0, #T_d1c3f_row8_col1, #T_d1c3f_row8_col2, #T_d1c3f_row9_col0, #T_d1c3f_row9_col1, #T_d1c3f_row9_col2, #T_d1c3f_row10_col0, #T_d1c3f_row10_col1, #T_d1c3f_row10_col2, #T_d1c3f_row11_col0, #T_d1c3f_row11_col1, #T_d1c3f_row11_col2, #T_d1c3f_row12_col0, #T_d1c3f_row12_col1, #T_d1c3f_row12_col2, #T_d1c3f_row13_col0, #T_d1c3f_row13_col1, #T_d1c3f_row13_col2, #T_d1c3f_row14_col0, #T_d1c3f_row14_col1, #T_d1c3f_row14_col2, #T_d1c3f_row15_col0, #T_d1c3f_row15_col1, #T_d1c3f_row15_col2, #T_d1c3f_row16_col0, #T_d1c3f_row16_col1, #T_d1c3f_row16_col2, #T_d1c3f_row17_col0, #T_d1c3f_row17_col1, #T_d1c3f_row17_col2, #T_d1c3f_row18_col0, #T_d1c3f_row18_col1, #T_d1c3f_row18_col2, #T_d1c3f_row19_col0, #T_d1c3f_row19_col1, #T_d1c3f_row19_col2 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_d1c3f\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_d1c3f_level0_col0\" class=\"col_heading level0 col0\" >Index</th>\n","      <th id=\"T_d1c3f_level0_col1\" class=\"col_heading level0 col1\" >Token_ID</th>\n","      <th id=\"T_d1c3f_level0_col2\" class=\"col_heading level0 col2\" >Decoded</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_d1c3f_row0_col0\" class=\"data row0 col0\" >1</td>\n","      <td id=\"T_d1c3f_row0_col1\" class=\"data row0 col1\" >13628</td>\n","      <td id=\"T_d1c3f_row0_col2\" class=\"data row0 col2\" >د</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_d1c3f_row1_col0\" class=\"data row1 col0\" >2</td>\n","      <td id=\"T_d1c3f_row1_col1\" class=\"data row1 col1\" >11318</td>\n","      <td id=\"T_d1c3f_row1_col2\" class=\"data row1 col2\" >ر</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_d1c3f_row2_col0\" class=\"data row2 col0\" >3</td>\n","      <td id=\"T_d1c3f_row2_col1\" class=\"data row2 col1\" >60942</td>\n","      <td id=\"T_d1c3f_row2_col2\" class=\"data row2 col2\" > س</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_d1c3f_row3_col0\" class=\"data row3 col0\" >4</td>\n","      <td id=\"T_d1c3f_row3_col1\" class=\"data row3 col1\" >32482</td>\n","      <td id=\"T_d1c3f_row3_col2\" class=\"data row3 col2\" >ال</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_d1c3f_row4_col0\" class=\"data row4 col0\" >5</td>\n","      <td id=\"T_d1c3f_row4_col1\" class=\"data row4 col1\" >90464</td>\n","      <td id=\"T_d1c3f_row4_col2\" class=\"data row4 col2\" >‌</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n","      <td id=\"T_d1c3f_row5_col0\" class=\"data row5 col0\" >6</td>\n","      <td id=\"T_d1c3f_row5_col1\" class=\"data row5 col1\" >16552</td>\n","      <td id=\"T_d1c3f_row5_col2\" class=\"data row5 col2\" >ه</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n","      <td id=\"T_d1c3f_row6_col0\" class=\"data row6 col0\" >7</td>\n","      <td id=\"T_d1c3f_row6_col1\" class=\"data row6 col1\" >47172</td>\n","      <td id=\"T_d1c3f_row6_col2\" class=\"data row6 col2\" >ای</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n","      <td id=\"T_d1c3f_row7_col0\" class=\"data row7 col0\" >8</td>\n","      <td id=\"T_d1c3f_row7_col1\" class=\"data row7 col1\" >13258</td>\n","      <td id=\"T_d1c3f_row7_col2\" class=\"data row7 col2\" > ا</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n","      <td id=\"T_d1c3f_row8_col0\" class=\"data row8 col0\" >9</td>\n","      <td id=\"T_d1c3f_row8_col1\" class=\"data row8 col1\" >36344</td>\n","      <td id=\"T_d1c3f_row8_col2\" class=\"data row8 col2\" >خ</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n","      <td id=\"T_d1c3f_row9_col0\" class=\"data row9 col0\" >10</td>\n","      <td id=\"T_d1c3f_row9_col1\" class=\"data row9 col1\" >90920</td>\n","      <td id=\"T_d1c3f_row9_col2\" class=\"data row9 col2\" >یر</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n","      <td id=\"T_d1c3f_row10_col0\" class=\"data row10 col0\" >11</td>\n","      <td id=\"T_d1c3f_row10_col1\" class=\"data row10 col1\" >69885</td>\n","      <td id=\"T_d1c3f_row10_col2\" class=\"data row10 col2\" >،</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n","      <td id=\"T_d1c3f_row11_col0\" class=\"data row11 col0\" >12</td>\n","      <td id=\"T_d1c3f_row11_col1\" class=\"data row11 col1\" >24252</td>\n","      <td id=\"T_d1c3f_row11_col2\" class=\"data row11 col2\" > م</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n","      <td id=\"T_d1c3f_row12_col0\" class=\"data row12 col0\" >13</td>\n","      <td id=\"T_d1c3f_row12_col1\" class=\"data row12 col1\" >13628</td>\n","      <td id=\"T_d1c3f_row12_col2\" class=\"data row12 col2\" >د</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n","      <td id=\"T_d1c3f_row13_col0\" class=\"data row13 col0\" >14</td>\n","      <td id=\"T_d1c3f_row13_col1\" class=\"data row13 col1\" >8700</td>\n","      <td id=\"T_d1c3f_row13_col2\" class=\"data row13 col2\" >ل</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n","      <td id=\"T_d1c3f_row14_col0\" class=\"data row14 col0\" >15</td>\n","      <td id=\"T_d1c3f_row14_col1\" class=\"data row14 col1\" >90464</td>\n","      <td id=\"T_d1c3f_row14_col2\" class=\"data row14 col2\" >‌</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n","      <td id=\"T_d1c3f_row15_col0\" class=\"data row15 col0\" >16</td>\n","      <td id=\"T_d1c3f_row15_col1\" class=\"data row15 col1\" >16552</td>\n","      <td id=\"T_d1c3f_row15_col2\" class=\"data row15 col2\" >ه</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n","      <td id=\"T_d1c3f_row16_col0\" class=\"data row16 col0\" >17</td>\n","      <td id=\"T_d1c3f_row16_col1\" class=\"data row16 col1\" >47172</td>\n","      <td id=\"T_d1c3f_row16_col2\" class=\"data row16 col2\" >ای</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n","      <td id=\"T_d1c3f_row17_col0\" class=\"data row17 col0\" >18</td>\n","      <td id=\"T_d1c3f_row17_col1\" class=\"data row17 col1\" >8979</td>\n","      <td id=\"T_d1c3f_row17_col2\" class=\"data row17 col2\" > �</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n","      <td id=\"T_d1c3f_row18_col0\" class=\"data row18 col0\" >19</td>\n","      <td id=\"T_d1c3f_row18_col1\" class=\"data row18 col1\" >110</td>\n","      <td id=\"T_d1c3f_row18_col2\" class=\"data row18 col2\" >�</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d1c3f_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n","      <td id=\"T_d1c3f_row19_col0\" class=\"data row19 col0\" >20</td>\n","      <td id=\"T_d1c3f_row19_col1\" class=\"data row19 col1\" >22071</td>\n","      <td id=\"T_d1c3f_row19_col2\" class=\"data row19 col2\" >ب</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7c46dddf9650>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import tiktoken\n","\n","# Load tokenizer\n","enc = tiktoken.get_encoding(\"cl100k_base\")\n","\n","# Your longer Persian paragraph\n","text_fa = (\n","    \"در سال‌های اخیر، مدل‌های زبانی پیشرفته نقش بسیار مهمی در تحولات فناوری ایفا کرده‌اند. \"\n","    \"این مدل‌ها اکنون قادرند متون پیچیده را درک کنند، به سؤالات پاسخ دهند و حتی متن‌هایی شبیه انسان تولید کنند. \"\n","    \"پیشرفت در این حوزه باعث بهبود چشمگیر در ترجمه ماشینی، دستیارهای صوتی و تحلیل داده‌های متنی شده است.\"\n",")\n","\n","# Tokenize and decode\n","token_ids = enc.encode(text_fa)\n","decoded_tokens = [enc.decode([t]) for t in token_ids]\n","\n","# Create concise DataFrame\n","df = pd.DataFrame({\n","    \"Index\": range(1, len(token_ids)+1),\n","    \"Token_ID\": token_ids,\n","    \"Decoded\": decoded_tokens\n","})\n","\n","print(f\"Total tokens: {len(token_ids)}\")\n","df.head(20).style.set_properties(**{\"text-align\": \"left\"})\n"]},{"cell_type":"markdown","id":"cd7d9536","metadata":{"papermill":{"duration":0.002803,"end_time":"2025-11-02T12:50:07.709863","exception":false,"start_time":"2025-11-02T12:50:07.70706","status":"completed"},"tags":[]},"source":["- It has about 207 tokens we are not gonna show all of them because makes the  notebook overwhelming."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":27.22231,"end_time":"2025-11-02T12:50:10.146719","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-02T12:49:42.924409","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}